{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tools, technologies, & techniques featured in this notebook\n",
    "- NLP preprocessing, clustering, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy.linalg import svd\n",
    "# import string\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# from nltk.stem.porter import PorterStemmer\n",
    "# from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing functions and methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/salvir1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet = WordNetLemmatizer()\n",
    "# porter = PorterStemmer()\n",
    "# snowball = SnowballStemmer('english')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_to_remove = ['ciambellone', 'ciambella','puff','croissant', 'croissants', 'crescent', 'brioche']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punc(string:str) -> str:\n",
    "    '''Given a string, removes all punctuation and returned punctuation-less string'''\n",
    "    return re.sub(r\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(str):\n",
    "    '''\n",
    "    Tokenize a str and return a tokenized list.\n",
    "    '''\n",
    "    return [word for word in word_tokenize(str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc):\n",
    "    '''Takes in a doc and lemmatizes tokens in doc\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    lemmatized tokens\n",
    "    '''\n",
    "    return [wordnet.lemmatize(tkn) for tkn in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_stop_words(doc, stops=set(stopwords.words('english'))):\n",
    "    '''Takes in a doc and removes stop words\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tokens with stop words removed\n",
    "    '''\n",
    "    return([w for w in doc if w not in stops])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rm_title_words(doc, title_words):\n",
    "    '''Takes in a doc and removes title words to allow algorithm to focus on other keywords\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc: list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tokens with title words removed\n",
    "    '''\n",
    "    return([w for w in doc if w not in title_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(content):\n",
    "    '''\n",
    "    Add docstring. Make flexible to allow for doing, or not doing, preprocessing functions. \n",
    "    Parameters\n",
    "    ----------\n",
    "    content (str): a collection of strings\n",
    "    Returns\n",
    "    -------\n",
    "    A list of lists: each list contains a tokenized version of the original string\n",
    "    '''\n",
    "    preprocessed = []\n",
    "    for i in range(len(content)):\n",
    "        step_1 = remove_punc(content[i].lower())\n",
    "        step_2 = tokenize(step_1)\n",
    "        step_3 = lemmatize(step_2)\n",
    "        step_4 = rm_stop_words(step_3)\n",
    "        step_5 = rm_title_words(step_4, titles_to_remove)\n",
    "        preprocessed.append(step_5)\n",
    "    return preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading sample data to check functions\n",
    "df_csnt = pd.read_csv('data/us_croissant.csv')\n",
    "df_ciambellone = pd.read_csv('data/us_ciambellone.csv')\n",
    "df_puff = pd.read_csv('data/us_puff.csv')\n",
    "df_brioche = pd.read_csv('data/us_brioche.csv')\n",
    "df = pd.concat([df_csnt, df_ciambellone, df_puff, df_brioche], axis=0, ignore_index=True) \n",
    "df.columns = ['drop','url','instructions','recipe type']\n",
    "df.drop('drop', axis = 1)\n",
    "corpus = df['instructions']\n",
    "y = df['recipe type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Watch the video above and use the step-by-step...\n",
       "1      For the dough: Put the eggs and water in a lar...\n",
       "2      YieldMakes 24 pastries\\nActive Time2 hr\\nTotal...\n",
       "3      It’s all about the layers…\\nKlik hier voor Ned...\n",
       "4      Combine all of the dough ingredients in the bo...\n",
       "                             ...                        \n",
       "265    In a medium mixing bowl, whisk together the wa...\n",
       "266    Combine 1/3 cup of milk and 1 tablespoon of fl...\n",
       "267    In a small bowl, whisk the yeast with the butt...\n",
       "268    Warm the milk, transfer to a bowl and crumble ...\n",
       "269    In a glass measuring cup, combine one cup warm...\n",
       "Name: instructions, Length: 270, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing--data load and function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_tokenized = preprocess_corpus(corpus) # cleaned and tokenized\n",
    "str_cleaned_tokenized = [\" \".join(x) for x in cleaned_tokenized] # string version of cleaned and tokenized "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<270x500 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 29000 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'Bag of words function'\n",
    "vect = CountVectorizer(max_features=500)\n",
    "word_counts = vect.fit_transform(str_cleaned_tokenized)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['10',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '18',\n",
       " '20',\n",
       " '23',\n",
       " '24',\n",
       " '25',\n",
       " '30',\n",
       " '34',\n",
       " '35',\n",
       " '350',\n",
       " '40',\n",
       " '400',\n",
       " '45',\n",
       " '60',\n",
       " '90',\n",
       " 'add',\n",
       " 'adding',\n",
       " 'air',\n",
       " 'allow',\n",
       " 'allpurpose',\n",
       " 'almond',\n",
       " 'almost',\n",
       " 'along',\n",
       " 'also',\n",
       " 'always',\n",
       " 'amount',\n",
       " 'another',\n",
       " 'apart',\n",
       " 'apple',\n",
       " 'approximately',\n",
       " 'around',\n",
       " 'arrange',\n",
       " 'aside',\n",
       " 'attachment',\n",
       " 'away',\n",
       " 'back',\n",
       " 'bag',\n",
       " 'bake',\n",
       " 'baked',\n",
       " 'baker',\n",
       " 'baking',\n",
       " 'ball',\n",
       " 'base',\n",
       " 'batch',\n",
       " 'batter',\n",
       " 'beat',\n",
       " 'become',\n",
       " 'begin',\n",
       " 'best',\n",
       " 'better',\n",
       " 'big',\n",
       " 'bit',\n",
       " 'block',\n",
       " 'book',\n",
       " 'bottom',\n",
       " 'bowl',\n",
       " 'bread',\n",
       " 'breakfast',\n",
       " 'bring',\n",
       " 'brown',\n",
       " 'brush',\n",
       " 'bundt',\n",
       " 'business',\n",
       " 'butter',\n",
       " 'buttery',\n",
       " 'cake',\n",
       " 'called',\n",
       " 'cant',\n",
       " 'carefully',\n",
       " 'center',\n",
       " 'check',\n",
       " 'cheese',\n",
       " 'chill',\n",
       " 'chilled',\n",
       " 'chocolate',\n",
       " 'classic',\n",
       " 'clean',\n",
       " 'cling',\n",
       " 'close',\n",
       " 'cm',\n",
       " 'coat',\n",
       " 'coconut',\n",
       " 'cold',\n",
       " 'combine',\n",
       " 'combined',\n",
       " 'come',\n",
       " 'completely',\n",
       " 'container',\n",
       " 'continue',\n",
       " 'cook',\n",
       " 'cooking',\n",
       " 'cool',\n",
       " 'cooling',\n",
       " 'corner',\n",
       " 'could',\n",
       " 'counter',\n",
       " 'couple',\n",
       " 'cover',\n",
       " 'covered',\n",
       " 'cream',\n",
       " 'create',\n",
       " 'creating',\n",
       " 'crust',\n",
       " 'cube',\n",
       " 'cup',\n",
       " 'cut',\n",
       " 'cutter',\n",
       " 'cutting',\n",
       " 'danish',\n",
       " 'day',\n",
       " 'deep',\n",
       " 'degree',\n",
       " 'delicious',\n",
       " 'depending',\n",
       " 'desired',\n",
       " 'difficult',\n",
       " 'direction',\n",
       " 'divide',\n",
       " 'doe',\n",
       " 'doesnt',\n",
       " 'done',\n",
       " 'dont',\n",
       " 'double',\n",
       " 'doubled',\n",
       " 'dough',\n",
       " 'dry',\n",
       " 'dtrempe',\n",
       " 'dust',\n",
       " 'easier',\n",
       " 'easy',\n",
       " 'eat',\n",
       " 'edge',\n",
       " 'egg',\n",
       " 'either',\n",
       " 'elastic',\n",
       " 'end',\n",
       " 'enjoy',\n",
       " 'enough',\n",
       " 'equal',\n",
       " 'even',\n",
       " 'evenly',\n",
       " 'every',\n",
       " 'everything',\n",
       " 'excess',\n",
       " 'extra',\n",
       " 'facing',\n",
       " 'fat',\n",
       " 'favorite',\n",
       " 'feel',\n",
       " 'filling',\n",
       " 'film',\n",
       " 'final',\n",
       " 'find',\n",
       " 'finger',\n",
       " 'finished',\n",
       " 'firm',\n",
       " 'first',\n",
       " 'fitted',\n",
       " 'flaky',\n",
       " 'flat',\n",
       " 'flatten',\n",
       " 'flavor',\n",
       " 'flip',\n",
       " 'flour',\n",
       " 'floured',\n",
       " 'foil',\n",
       " 'fold',\n",
       " 'folded',\n",
       " 'folding',\n",
       " 'food',\n",
       " 'fork',\n",
       " 'form',\n",
       " 'found',\n",
       " 'four',\n",
       " 'freeze',\n",
       " 'freezer',\n",
       " 'french',\n",
       " 'fresh',\n",
       " 'fridge',\n",
       " 'frozen',\n",
       " 'full',\n",
       " 'fully',\n",
       " 'gently',\n",
       " 'get',\n",
       " 'give',\n",
       " 'gluten',\n",
       " 'go',\n",
       " 'going',\n",
       " 'golden',\n",
       " 'good',\n",
       " 'gram',\n",
       " 'grease',\n",
       " 'greased',\n",
       " 'great',\n",
       " 'ha',\n",
       " 'half',\n",
       " 'hand',\n",
       " 'hard',\n",
       " 'heat',\n",
       " 'heavy',\n",
       " 'help',\n",
       " 'high',\n",
       " 'hold',\n",
       " 'home',\n",
       " 'homemade',\n",
       " 'hook',\n",
       " 'hot',\n",
       " 'hour',\n",
       " 'however',\n",
       " 'ice',\n",
       " 'im',\n",
       " 'important',\n",
       " 'inch',\n",
       " 'incorporated',\n",
       " 'increase',\n",
       " 'ingredient',\n",
       " 'inside',\n",
       " 'instant',\n",
       " 'instead',\n",
       " 'instruction',\n",
       " 'italian',\n",
       " 'ive',\n",
       " 'juice',\n",
       " 'keep',\n",
       " 'keeping',\n",
       " 'kitchen',\n",
       " 'knead',\n",
       " 'kneading',\n",
       " 'knife',\n",
       " 'know',\n",
       " 'laminated',\n",
       " 'laminating',\n",
       " 'lamination',\n",
       " 'large',\n",
       " 'last',\n",
       " 'later',\n",
       " 'layer',\n",
       " 'le',\n",
       " 'least',\n",
       " 'leave',\n",
       " 'left',\n",
       " 'lemon',\n",
       " 'length',\n",
       " 'lengthwise',\n",
       " 'let',\n",
       " 'letter',\n",
       " 'light',\n",
       " 'lightly',\n",
       " 'like',\n",
       " 'line',\n",
       " 'lined',\n",
       " 'little',\n",
       " 'loaf',\n",
       " 'long',\n",
       " 'longer',\n",
       " 'look',\n",
       " 'loosely',\n",
       " 'lot',\n",
       " 'love',\n",
       " 'low',\n",
       " 'lower',\n",
       " 'made',\n",
       " 'make',\n",
       " 'making',\n",
       " 'many',\n",
       " 'margarine',\n",
       " 'mark',\n",
       " 'mat',\n",
       " 'may',\n",
       " 'mean',\n",
       " 'measure',\n",
       " 'medium',\n",
       " 'melt',\n",
       " 'method',\n",
       " 'middle',\n",
       " 'might',\n",
       " 'milk',\n",
       " 'minute',\n",
       " 'mix',\n",
       " 'mixer',\n",
       " 'mixing',\n",
       " 'mixture',\n",
       " 'month',\n",
       " 'much',\n",
       " 'must',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'new',\n",
       " 'next',\n",
       " 'nice',\n",
       " 'note',\n",
       " 'oil',\n",
       " 'olive',\n",
       " 'one',\n",
       " 'onto',\n",
       " 'open',\n",
       " 'orange',\n",
       " 'ounce',\n",
       " 'oven',\n",
       " 'overnight',\n",
       " 'package',\n",
       " 'paddle',\n",
       " 'pan',\n",
       " 'paper',\n",
       " 'parchment',\n",
       " 'part',\n",
       " 'pastry',\n",
       " 'perfect',\n",
       " 'photo',\n",
       " 'pie',\n",
       " 'piece',\n",
       " 'pin',\n",
       " 'pinch',\n",
       " 'pizza',\n",
       " 'place',\n",
       " 'plain',\n",
       " 'plastic',\n",
       " 'pliable',\n",
       " 'plus',\n",
       " 'point',\n",
       " 'portion',\n",
       " 'possible',\n",
       " 'post',\n",
       " 'pound',\n",
       " 'pour',\n",
       " 'powder',\n",
       " 'preheat',\n",
       " 'preheated',\n",
       " 'prepare',\n",
       " 'prepared',\n",
       " 'press',\n",
       " 'pretty',\n",
       " 'prevent',\n",
       " 'print',\n",
       " 'process',\n",
       " 'processor',\n",
       " 'proof',\n",
       " 'proofing',\n",
       " 'put',\n",
       " 'quick',\n",
       " 'quickly',\n",
       " 'quite',\n",
       " 'rack',\n",
       " 'read',\n",
       " 'ready',\n",
       " 'really',\n",
       " 'recipe',\n",
       " 'recommend',\n",
       " 'rectangle',\n",
       " 'refrigerate',\n",
       " 'refrigerator',\n",
       " 'remaining',\n",
       " 'remember',\n",
       " 'remove',\n",
       " 'repeat',\n",
       " 'rest',\n",
       " 'result',\n",
       " 'return',\n",
       " 'right',\n",
       " 'ring',\n",
       " 'rise',\n",
       " 'roll',\n",
       " 'rolled',\n",
       " 'rolling',\n",
       " 'room',\n",
       " 'rotate',\n",
       " 'rough',\n",
       " 'ruler',\n",
       " 'run',\n",
       " 'salt',\n",
       " 'say',\n",
       " 'scrape',\n",
       " 'scraper',\n",
       " 'seal',\n",
       " 'seam',\n",
       " 'second',\n",
       " 'see',\n",
       " 'serve',\n",
       " 'set',\n",
       " 'several',\n",
       " 'shape',\n",
       " 'shaped',\n",
       " 'shaping',\n",
       " 'share',\n",
       " 'sharp',\n",
       " 'sheet',\n",
       " 'short',\n",
       " 'side',\n",
       " 'simple',\n",
       " 'since',\n",
       " 'single',\n",
       " 'sit',\n",
       " 'size',\n",
       " 'slice',\n",
       " 'slightly',\n",
       " 'slowly',\n",
       " 'small',\n",
       " 'smooth',\n",
       " 'soft',\n",
       " 'something',\n",
       " 'spatula',\n",
       " 'speed',\n",
       " 'spoon',\n",
       " 'spray',\n",
       " 'spread',\n",
       " 'spring',\n",
       " 'sprinkle',\n",
       " 'square',\n",
       " 'stand',\n",
       " 'start',\n",
       " 'starting',\n",
       " 'stay',\n",
       " 'steam',\n",
       " 'step',\n",
       " 'stick',\n",
       " 'sticking',\n",
       " 'sticky',\n",
       " 'still',\n",
       " 'stir',\n",
       " 'store',\n",
       " 'straight',\n",
       " 'stretch',\n",
       " 'sugar',\n",
       " 'sure',\n",
       " 'surface',\n",
       " 'sweet',\n",
       " 'tablespoon',\n",
       " 'take',\n",
       " 'taste',\n",
       " 'teaspoon',\n",
       " 'technique',\n",
       " 'temperature',\n",
       " 'texture',\n",
       " 'thats',\n",
       " 'thaw',\n",
       " 'thick',\n",
       " 'thin',\n",
       " 'thing',\n",
       " 'think',\n",
       " 'third',\n",
       " 'though',\n",
       " 'three',\n",
       " 'tightly',\n",
       " 'time',\n",
       " 'tin',\n",
       " 'tip',\n",
       " 'together',\n",
       " 'top',\n",
       " 'total',\n",
       " 'towards',\n",
       " 'towel',\n",
       " 'transfer',\n",
       " 'tray',\n",
       " 'triangle',\n",
       " 'try',\n",
       " 'tsp',\n",
       " 'turn',\n",
       " 'twice',\n",
       " 'two',\n",
       " 'type',\n",
       " 'unsalted',\n",
       " 'use',\n",
       " 'used',\n",
       " 'using',\n",
       " 'usually',\n",
       " 'vanilla',\n",
       " 'vegan',\n",
       " 'version',\n",
       " 'wa',\n",
       " 'want',\n",
       " 'warm',\n",
       " 'wash',\n",
       " 'water',\n",
       " 'way',\n",
       " 'well',\n",
       " 'whisk',\n",
       " 'white',\n",
       " 'whole',\n",
       " 'wide',\n",
       " 'wire',\n",
       " 'without',\n",
       " 'wont',\n",
       " 'work',\n",
       " 'working',\n",
       " 'would',\n",
       " 'wrap',\n",
       " 'year',\n",
       " 'yeast',\n",
       " 'yogurt',\n",
       " 'yolk',\n",
       " 'youll',\n",
       " 'youre',\n",
       " 'youve',\n",
       " 'zest']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.01336513, 0.02400422, ..., 0.01998364, 0.        ,\n",
       "        0.        ],\n",
       "       [0.08770108, 0.0568134 , 0.05101939, ..., 0.        , 0.05036798,\n",
       "        0.        ],\n",
       "       [0.11471624, 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.10225533, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.1151328 , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfvect = TfidfVectorizer(max_features=500)\n",
    "tfidf_vectorized = tfidfvect.fit_transform(str_cleaned_tokenized)\n",
    "tfidf_vectorized.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of two common naive Bayes variants used in text classification\n",
    "- Usually done on word vector counts, but can also be done on tf-idf vectors\n",
    "- The distribution is parametrized by theta_y vectors for each class\n",
    "- theta_y is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(word_counts, y, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean accuracy: 0.8676470588235294\n",
      "\n",
      "     Prediction:          Actual                Prob brioche     Prob ciam     Prob crsnt      Prob puff \n",
      "     ----------           ------                ------------     ---------     ----------      ---------\n",
      "P:  brioche         A:  ciambellone        Probs:   0.998          0.000          0.002          0.000\n",
      "P:  brioche         A:  puff_pastry        Probs:   1.000          0.000          0.000          0.000\n",
      "P:  brioche         A:  croissant          Probs:   0.988          0.005          0.007          0.000\n",
      "P:  brioche         A:  croissant          Probs:   1.000          0.000          0.000          0.000\n",
      "P:  croissant       A:  puff_pastry        Probs:   0.463          0.000          0.537          0.000\n",
      "P:  brioche         A:  ciambellone        Probs:   1.000          0.000          0.000          0.000\n",
      "P:  ciambellone     A:  puff_pastry        Probs:   0.000          1.000          0.000          0.000\n",
      "P:  brioche         A:  puff_pastry        Probs:   1.000          0.000          0.000          0.000\n",
      "P:  croissant       A:  brioche            Probs:   0.000          0.000          1.000          0.000\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "scoreboard = zip(clf.predict_proba(X_test), clf.predict(X_test), y_test)\n",
    "spc=''\n",
    "print(f'\\nMean accuracy: {clf.score(X_test, y_test)}\\n')\n",
    "\n",
    "print(f'     Prediction:          Actual                Prob brioche     Prob ciam     Prob crsnt      Prob puff ')\n",
    "print(f'     ----------           ------                ------------     ---------     ----------      ---------')\n",
    "for i in scoreboard:\n",
    "    if i[2] != i[1]:\n",
    "        print(f'P:  {i[1]:<15} A:  {i[2]:<18} Probs:   {i[0][0]:.3f}{spc:<10}{i[0][1]:.3f}{spc:<10}{i[0][2]:.3f}{spc:<10}{i[0][3]:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion matrix. Predictions in rows. Actuals in columns\n",
      "\n",
      "\t\t     Br  Cm  Cs  Pf\n",
      "\t\t     --  --  --  --\n",
      "\t\tBrio [12  2  2  2]\n",
      "\t\tCiam [ 0 11  0  1]\n",
      "\t\tCsnt [ 1  0 20  1]\n",
      "\t\tPuff [ 0  0  0 16]\n",
      "\n",
      "Mean accuracy: 0.8676470588235294\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'\\nConfusion matrix. Predictions in rows. Actuals in columns\\n')\n",
    "\n",
    "print(f'\\t\\t     Br  Cm  Cs  Pf')\n",
    "print(f'\\t\\t     --  --  --  --')\n",
    "print(f'\\t\\tBrio {confusion_matrix( clf.predict(X_test), y_test)[0]}')\n",
    "print(f'\\t\\tCiam {confusion_matrix( clf.predict(X_test), y_test)[1]}')\n",
    "print(f'\\t\\tCsnt {confusion_matrix( clf.predict(X_test), y_test)[2]}')\n",
    "print(f'\\t\\tPuff {confusion_matrix( clf.predict(X_test), y_test)[3]}')\n",
    "\n",
    "print(f'\\nMean accuracy: {clf.score(X_test, y_test)}\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8814814814814815\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True)  # almost always use shuffle=True\n",
    "fold_scores = []\n",
    "\n",
    "for train, test in kf.split(word_counts):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(word_counts[train], y[train])\n",
    "    fold_scores.append(model.score(word_counts[test], y[test]))\n",
    "    \n",
    "print(np.mean(fold_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      Watch the video above and use the step-by-step...\n",
       "1      For the dough: Put the eggs and water in a lar...\n",
       "2      YieldMakes 24 pastries\\nActive Time2 hr\\nTotal...\n",
       "3      It’s all about the layers…\\nKlik hier voor Ned...\n",
       "4      Combine all of the dough ingredients in the bo...\n",
       "                             ...                        \n",
       "265    In a medium mixing bowl, whisk together the wa...\n",
       "266    Combine 1/3 cup of milk and 1 tablespoon of fl...\n",
       "267    In a small bowl, whisk the yeast with the butt...\n",
       "268    Warm the milk, transfer to a bowl and crumble ...\n",
       "269    In a glass measuring cup, combine one cup warm...\n",
       "Name: instructions, Length: 270, dtype: object"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(cleaned_tokenized)\n",
    "#A value of 2 for min_count specifies to include only those words in the Word2Vec model that appear at least twice in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = word2vec.wv.vocab\n",
    "#vocabulary = word2vec objects containing words appearing two or more times in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stand', 0.9823739528656006),\n",
       " ('large', 0.9782915115356445),\n",
       " ('mixer', 0.9734643697738647),\n",
       " ('speed', 0.9653637409210205),\n",
       " ('hook', 0.9618068933486938),\n",
       " ('medium', 0.9615850448608398),\n",
       " ('fitted', 0.9613025188446045),\n",
       " ('beat', 0.9564694166183472),\n",
       " ('mixing', 0.9548104405403137),\n",
       " ('whisk', 0.9545235633850098)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2vec.wv.most_similar('bowl')\n",
    "# method that returns the most similar words to the given word, not association, but similarity supposedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering with K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = 8\n",
    "kmeans = KMeans(n_clusters=clusters, \n",
    "                random_state=0).fit(tfidf_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Investigate the clusters  \n",
    "\n",
    "> - Investigate the 'centroids' to find out what \"topics\" Kmeans has discovered by mapping these vectors back into the 'word space'.  Think of each feature/dimension of the centroid vector as representing the \"average\" article or the average occurrences of words for that cluster.\n",
    "   \n",
    "> - Find the features/dimensions with the greatest representation in the centroid.  Print out the top ten words for each centroid.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pastry dough puff butter flour roll fold make recipe water cold rolling \n",
      "\n",
      "dough butter flour wrap roll rectangle fold plastic work water third surface \n",
      "\n",
      "batter pan cake sugar bundt minute flour oil beat whisk preheat egg \n",
      "\n"
     ]
    }
   ],
   "source": [
    "def Sort(sub_li): \n",
    "    return sorted(sub_li, key = lambda x: x[0], reverse=True)\n",
    "\n",
    "def get_word(centroid):\n",
    "    return [x[1] for x in centroid]\n",
    "\n",
    "for k in range(3):\n",
    "    matched = zip(kmeans.cluster_centers_[k], tfidfvect.get_feature_names())\n",
    "    match = Sort(list(matched))\n",
    "    print(' '.join(get_word(match[:12])), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For heirarchical clustering methods, see 819 am clustering assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "- Unsupervised learning\n",
    "\n",
    "- Use the cosine similarity to compare similarity between documents.\n",
    "\n",
    "- sklearn's [linear_kernel](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.linear_kernel.html) (computes dot product) can be used on tfidf to compute the cosine similarity since rows are normalized.*\n",
    "\n",
    "- Here's a page on cosine similarity from [sklearn documentation](http://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) and a relevant [stack overflow post](http://stackoverflow.com/questions/12118720/python-tf-idf-cosine-to-find-document-similarity).\n",
    "\n",
    "- *The stack overflow post is helpful. It provides instruction over how to slice the tfidf and then how to apply cosine similarity between one doc and all of the rest.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.42532042, 0.51480273, 0.63426294, 0.58131785,\n",
       "       0.49458012, 0.41182741, 0.67212269, 0.39923599, 0.43034994,\n",
       "       0.59669337, 0.55242081, 0.39986244, 0.59749017, 0.59781199,\n",
       "       0.61531608, 0.56630834, 0.48291278, 0.51860656, 0.62124648,\n",
       "       0.55917093, 0.64123624, 0.62018231, 0.21397562, 0.44491723,\n",
       "       0.66974737, 0.51849926, 0.60090852, 0.56011136, 0.5395997 ,\n",
       "       0.33038423, 0.32047009, 0.61687334, 0.22388983, 0.59963174,\n",
       "       0.46037651, 0.54202385, 0.50374599, 0.54340846, 0.55421876,\n",
       "       0.26610053, 0.56702984, 0.54851483, 0.29379981, 0.60718011,\n",
       "       0.52875962, 0.16581074, 0.19842094, 0.64240339, 0.53090086,\n",
       "       0.33606642, 0.22834341, 0.54317563, 0.61562137, 0.19273947,\n",
       "       0.20738625, 0.43520125, 0.208638  , 0.22270445, 0.61583263,\n",
       "       0.18295336, 0.56707674, 0.51721626, 0.54509692, 0.60539148,\n",
       "       0.6357192 , 0.50188313, 0.15531057, 0.53276468, 0.53276468,\n",
       "       0.1737135 , 0.32499931, 0.1876329 , 0.15935641, 0.14880965,\n",
       "       0.20381423, 0.32924426, 0.18180539, 0.13502847, 0.12822435,\n",
       "       0.40352382, 0.16515378, 0.17199654, 0.12800899, 0.13490661,\n",
       "       0.13918647, 0.18152627, 0.18505195, 0.11190134, 0.17604608,\n",
       "       0.09892148, 0.2136355 , 0.15277539, 0.10293867, 0.09381505,\n",
       "       0.07266629, 0.22552622, 0.43268307, 0.06224791, 0.36114088,\n",
       "       0.06978591, 0.2117522 , 0.20578299, 0.14622832, 0.07995152,\n",
       "       0.19954369, 0.26586996, 0.13922436, 0.16568901, 0.17099357,\n",
       "       0.08407929, 0.16004871, 0.28303389, 0.15687965, 0.19075401,\n",
       "       0.16618235, 0.09387728, 0.47406492, 0.15729701, 0.19012458,\n",
       "       0.1386011 , 0.        , 0.20546524, 0.14884065, 0.1594825 ,\n",
       "       0.16761153, 0.12388735, 0.04026846, 0.64343882, 0.49008245,\n",
       "       0.52569001, 0.64343882, 0.47417022, 0.21470616, 0.52800523,\n",
       "       0.436216  , 0.43882107, 0.400262  , 0.45079802, 0.45509184,\n",
       "       0.52344678, 0.45147361, 0.46912104, 0.55318171, 0.33051088,\n",
       "       0.39356824, 0.6100467 , 0.49220767, 0.29543969, 0.61431487,\n",
       "       0.52103967, 0.49168836, 0.45132197, 0.52781172, 0.34652751,\n",
       "       0.48302434, 0.59440438, 0.23984541, 0.23984541, 0.51349841,\n",
       "       0.48566617, 0.34236661, 0.53307429, 0.45229489, 0.46579824,\n",
       "       0.19075085, 0.15727191, 0.44619347, 0.50548684, 0.2674396 ,\n",
       "       0.51500748, 0.59599684, 0.35934149, 0.45953622, 0.21886757,\n",
       "       0.37646579, 0.50389722, 0.552398  , 0.28598351, 0.45123011,\n",
       "       0.46392241, 0.3842556 , 0.22558659, 0.36559225, 0.2896297 ,\n",
       "       0.49503438, 0.28933565, 0.51680995, 0.51335305, 0.420538  ,\n",
       "       0.27246536, 0.43282445, 0.27834187, 0.48727889, 0.44384568,\n",
       "       0.44950365, 0.58131785, 0.43034349, 0.42625824, 0.49079169,\n",
       "       0.24831356, 0.51738459, 0.52715122, 0.46953643, 0.56401692,\n",
       "       0.16775526, 0.39218661, 0.50837391, 0.33498987, 0.14378087,\n",
       "       0.14432937, 0.37779207, 0.11864786, 0.43976886])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities = linear_kernel(tfidf_vectorized[1:2], tfidf_vectorized[1:500]).flatten() # This is comparing cs for article #2 and the next 500.\n",
    "cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   7  25 131]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.        , 0.67212269, 0.66974737, 0.64343882])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "related_docs_indices = cosine_similarities.argsort()[:-5:-1] # This identifies the index of the top 5 most similar.\n",
    "print(related_docs_indices)\n",
    "\n",
    "cosine_similarities[related_docs_indices] # and their related cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles.iloc[411] # Going step by step pulling up the most similar articles by index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decompositions NMF (and SVD)\n",
    "- Unsupervised learning\n",
    "- Good for situations when there's some potentially valid grouping to both rows and columns, such as putting Joe and Sam in the same group because they like similar movies (as opposed to traditional supervised models where there are features and targets)\n",
    "- See 820pm solution to NMF for good soft classification and test of classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes\n",
    "- Supervised learning method to assign class probabilities to a document\n",
    "- See 818PM NLP-pipeline-programming-net-example for using sklearn Naive Bayes classifier. See also 818PM lecture on text classification. Solutions to assignment contain a number of useful naive Bayes python functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
